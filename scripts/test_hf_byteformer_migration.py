#!/usr/bin/env python3
"""
ByteFormer HuggingFaceè¿ç§»è„šæœ¬
ç›´æ¥åŠ è½½CoreNetæ¨¡å‹å¹¶å°è£…ä¸ºHFæ¨¡å‹ï¼Œç¡®ä¿èƒ½åŠ è½½é¢„è®­ç»ƒæƒé‡
"""
import torch
import argparse
from corenet.options.opts import get_training_arguments
from corenet.utils.hf_adapter_utils import CorenetToHFPretrainedConfig, CorenetToHFPretrainedModel

def main():
    print("=== ByteFormer åˆ° HuggingFace æ¡†æ¶è¿ç§» ===\n")
    
    # 1. ä½¿ç”¨çœŸå®çš„é…ç½®æ–‡ä»¶è·¯å¾„
    config_file = "/root/autodl-tmp/corenet/projects/byteformer/imagenet_jpeg_q60/conv_kernel_size=4,window_sizes=[128].yaml"
    
    # 2. æ¨¡æ‹Ÿå‘½ä»¤è¡Œå‚æ•°
    args = [
        "--common.config-file", config_file,
        "--model.classification.pretrained", "/root/autodl-tmp/corenet/weights/imagenet_jpeg_q60_k4_w128.pt"
    ]
    
    # 3. è·å–å®Œæ•´çš„CoreNeté…ç½®
    print("åŠ è½½CoreNeté…ç½®...")
    opts = get_training_arguments(args=args)
    print("âœ“ é…ç½®åŠ è½½æˆåŠŸ")
    
    # 4. åˆ›å»ºHuggingFaceé€‚é…å™¨é…ç½®
    print("åˆ›å»ºHuggingFaceé€‚é…å™¨é…ç½®...")
    hf_config = CorenetToHFPretrainedConfig(**vars(opts))
    
    # 5. åˆ›å»ºHFå°è£…çš„æ¨¡å‹
    print("åˆ›å»ºHuggingFaceå°è£…æ¨¡å‹...")
    vocab_size = getattr(opts, "model.classification.byteformer.vocab_size")
    model = CorenetToHFPretrainedModel(hf_config, vocab_size)
    print(f"âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œvocab_size: {vocab_size}")
    
    # 6. æ£€æŸ¥æ¨¡å‹ç»“æ„
    print(f"âœ“ åµŒå…¥ç»´åº¦: {model.model.embeddings.embedding_dim}")
    print(f"âœ“ å·ç§¯æ ¸å¤§å°: {model.model.conv_kernel_size}")
    print(f"âœ“ æœ€å¤§tokenæ•°: {model.model.max_num_tokens}")
    
    # 7. åŠ è½½é¢„è®­ç»ƒæƒé‡
    print("\nåŠ è½½é¢„è®­ç»ƒæƒé‡...")
    weights_path = "/root/autodl-tmp/corenet/weights/imagenet_jpeg_q60_k4_w128.pt"
    weights = torch.load(weights_path, map_location='cpu')
    
    # 8. ç›´æ¥åŠ è½½æƒé‡åˆ°CoreNetæ¨¡å‹éƒ¨åˆ†
    model.model.load_state_dict(weights, strict=True)
    print("âœ“ é¢„è®­ç»ƒæƒé‡åŠ è½½æˆåŠŸï¼")
    
    # 9. è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    
    # 10. æµ‹è¯•å‰å‘æ¨ç†
    print("\nè¿›è¡Œå‰å‘æ¨ç†æµ‹è¯•...")
    test_length = 1000  # ä½¿ç”¨è¾ƒå°çš„é•¿åº¦é¿å…å†…å­˜é—®é¢˜
    input_ids = torch.randint(0, vocab_size-1, (1, test_length))
    
    print(f"æµ‹è¯•è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
    
    with torch.no_grad():
        try:
            output = model(input_ids=input_ids)
            print("âœ“ å‰å‘æ¨ç†æˆåŠŸï¼")
            print(f"  è¾“å‡ºlogitså½¢çŠ¶: {output.logits.shape}")
            print(f"  é¢„æµ‹ç±»åˆ«: {torch.argmax(output.logits, dim=-1).item()}")
            print(f"  æœ€å¤§æ¦‚ç‡: {torch.max(torch.softmax(output.logits, dim=-1)).item():.4f}")
            
            if output.past_key_values is not None:
                print(f"  KVç¼“å­˜: {len(output.past_key_values)} å±‚")
                
        except Exception as e:
            print(f"âœ— å‰å‘æ¨ç†å¤±è´¥: {e}")
            return False
    
    # 11. ä¿å­˜HFæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
    print("\nä¿å­˜HuggingFaceæ ¼å¼æ¨¡å‹...")
    save_dir = "/root/autodl-tmp/corenet/hf_byteformer_model"
    try:
        model.save_pretrained(save_dir)
        hf_config.save_pretrained(save_dir)
        print(f"âœ“ HFæ¨¡å‹å·²ä¿å­˜åˆ°: {save_dir}")
    except Exception as e:
        print(f"âš  æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
    
    print("\n=== è¿ç§»å®Œæˆ ===")
    print("âœ“ ByteFormerå·²æˆåŠŸè¿ç§»åˆ°HuggingFaceæ¡†æ¶")
    print("âœ“ é¢„è®­ç»ƒæƒé‡å·²æ­£ç¡®åŠ è½½")
    print("âœ“ æ¨¡å‹å¯æ­£å¸¸è¿›è¡Œå‰å‘æ¨ç†")
    print("\nä½ ç°åœ¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹å¼åŠ è½½æ¨¡å‹:")
    print("  from transformers import AutoModel, AutoConfig")
    print(f"  model = AutoModel.from_pretrained('{save_dir}')")
    
    return True

if __name__ == "__main__":
    success = main()
    if success:
        print("\nğŸ‰ è¿ç§»æˆåŠŸï¼")
    else:
        print("\nâŒ è¿ç§»å¤±è´¥ï¼")
