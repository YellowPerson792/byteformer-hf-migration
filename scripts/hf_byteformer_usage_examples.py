#!/usr/bin/env python3
"""
ByteFormer HuggingFaceä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•åœ¨HFæ¡†æ¶ä¸‹ä½¿ç”¨å·²è¿ç§»çš„ByteFormeræ¨¡å‹
"""
import sys
import os
from pathlib import Path

# æ·»åŠ utilsè·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir.parent))

# è®¾ç½®CoreNetè·¯å¾„å’Œæ£€æŸ¥ä¾èµ–
from utils.path_config import setup_corenet_path, get_config_file_path, get_weights_file_path, check_dependencies

# åˆå§‹åŒ–è·¯å¾„é…ç½®
try:
    check_dependencies()
    setup_corenet_path()
except Exception as e:
    print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
    sys.exit(1)

import torch
from corenet.options.opts import get_training_arguments
from corenet.utils.hf_adapter_utils import CorenetToHFPretrainedConfig, CorenetToHFPretrainedModel

def load_byteformer_hf_model():
    """åŠ è½½å·²è¿ç§»çš„ByteFormer HFæ¨¡å‹"""
    print("=== åŠ è½½ByteFormer HuggingFaceæ¨¡å‹ ===\n")
    
    # 1. ä½¿ç”¨åŠ¨æ€è·¯å¾„é…ç½®
    try:
        config_file = get_config_file_path()
        weights_file = get_weights_file_path()
    except FileNotFoundError as e:
        print(f"âŒ æ–‡ä»¶è·¯å¾„é”™è¯¯: {e}")
        return None, None
    
    # 2. è·å–CoreNeté…ç½®
    args = [
        "--common.config-file", config_file,
        "--model.classification.pretrained", weights_file
    ]
    opts = get_training_arguments(args=args)
    
    # 3. åˆ›å»ºHFæ¨¡å‹
    hf_config = CorenetToHFPretrainedConfig(**vars(opts))
    vocab_size = getattr(opts, "model.classification.byteformer.vocab_size")
    model = CorenetToHFPretrainedModel(hf_config, vocab_size)
    
    # 4. åŠ è½½é¢„è®­ç»ƒæƒé‡
    weights = torch.load(weights_file, map_location='cpu')
    model.model.load_state_dict(weights, strict=True)
    model.eval()
    
    print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼Œvocab_size: {vocab_size}")
    return model, vocab_size

def example_inference(model, vocab_size):
    """æ¨ç†ç¤ºä¾‹"""
    print("\n=== æ¨ç†ç¤ºä¾‹ ===")
    
    # æ¨¡æ‹Ÿå­—èŠ‚åºåˆ—è¾“å…¥ï¼ˆä¾‹å¦‚å›¾åƒçš„å­—èŠ‚è¡¨ç¤ºï¼‰
    batch_size = 2
    seq_length = 1000
    input_ids = torch.randint(0, vocab_size-1, (batch_size, seq_length))
    
    print(f"è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
    
    with torch.no_grad():
        outputs = model(input_ids=input_ids)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=-1)
        probabilities = torch.softmax(logits, dim=-1)
        max_probs = torch.max(probabilities, dim=-1).values
        
        print(f"è¾“å‡ºlogitså½¢çŠ¶: {logits.shape}")
        print(f"é¢„æµ‹ç±»åˆ«: {predictions.tolist()}")
        print(f"æœ€å¤§æ¦‚ç‡: {max_probs.tolist()}")

def example_text_generation(model, vocab_size):
    """æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹ï¼ˆKVç¼“å­˜ï¼‰"""
    print("\n=== ç”Ÿæˆç¤ºä¾‹ï¼ˆKVç¼“å­˜ï¼‰===")
    
    # åˆå§‹è¾“å…¥
    prefix_length = 100
    input_ids = torch.randint(0, vocab_size-1, (1, prefix_length))
    
    print(f"åˆå§‹è¾“å…¥é•¿åº¦: {prefix_length}")
    
    # ç¬¬ä¸€æ¬¡å‰å‘ï¼ˆå¤„ç†å‰ç¼€ï¼‰
    with torch.no_grad():
        outputs = model(input_ids=input_ids, use_cache=True)
        past_key_values = outputs.past_key_values
        next_token_logits = outputs.logits[:, -1, :]
        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
        
        print(f"âœ“ å‰ç¼€å¤„ç†å®Œæˆï¼ŒKVç¼“å­˜: {len(past_key_values)} å±‚")
        
        # ç»§ç»­ç”Ÿæˆï¼ˆä½¿ç”¨KVç¼“å­˜ï¼‰
        generated_tokens = [next_token.item()]
        current_input = next_token
        
        for step in range(5):  # ç”Ÿæˆ5ä¸ªtoken
            outputs = model(
                input_ids=current_input,
                past_key_values=past_key_values,
                use_cache=True
            )
            past_key_values = outputs.past_key_values
            next_token_logits = outputs.logits[:, -1, :]
            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)
            generated_tokens.append(next_token.item())
            current_input = next_token
        
        print(f"âœ“ ç”Ÿæˆå®Œæˆï¼Œæ–°tokens: {generated_tokens}")

def example_training_setup(model):
    """è®­ç»ƒè®¾ç½®ç¤ºä¾‹"""
    print("\n=== è®­ç»ƒè®¾ç½®ç¤ºä¾‹ ===")
    
    # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    model.train()
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    criterion = torch.nn.CrossEntropyLoss()
    
    print("âœ“ è®­ç»ƒç»„ä»¶è®¾ç½®å®Œæˆ")
    print(f"  ä¼˜åŒ–å™¨: {type(optimizer).__name__}")
    print(f"  æŸå¤±å‡½æ•°: {type(criterion).__name__}")
    print(f"  å¯è®­ç»ƒå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    
    # æ¨¡æ‹Ÿä¸€ä¸ªè®­ç»ƒæ­¥éª¤
    model.eval()  # æ¢å¤è¯„ä¼°æ¨¡å¼
    
    return optimizer, criterion

def main():
    """ä¸»å‡½æ•°"""
    print("ByteFormer HuggingFaceæ¡†æ¶ä½¿ç”¨ç¤ºä¾‹\n")
    
    try:
        # 1. åŠ è½½æ¨¡å‹
        model, vocab_size = load_byteformer_hf_model()
        
        # 2. æ¨ç†ç¤ºä¾‹
        example_inference(model, vocab_size)
        
        # 3. ç”Ÿæˆç¤ºä¾‹
        example_text_generation(model, vocab_size)
        
        # 4. è®­ç»ƒè®¾ç½®ç¤ºä¾‹
        optimizer, criterion = example_training_setup(model)
        
        print("\n=== æ€»ç»“ ===")
        print("âœ“ ByteFormerå·²æˆåŠŸè¿ç§»åˆ°HuggingFaceæ¡†æ¶")
        print("âœ“ æ”¯æŒæ ‡å‡†çš„forwardæ¨ç†")
        print("âœ“ æ”¯æŒKVç¼“å­˜çš„å¢é‡ç”Ÿæˆ")
        print("âœ“ å¯ç”¨äºHuggingFace Trainerè®­ç»ƒ")
        print("âœ“ å…¼å®¹æ‰€æœ‰HuggingFaceç”Ÿæ€å·¥å…·")
        
        print("\nğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡ŒæˆåŠŸï¼")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
