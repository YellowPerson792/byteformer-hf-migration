class MySeq2SeqTrainer:
    def save_model(self, output_dir=None):
        """æ‰‹åŠ¨ä¿å­˜å½“å‰æ¨¡å‹å’Œåˆ†è¯å™¨"""
        if output_dir is None:
            output_dir = self.args.output_dir
        os.makedirs(output_dir, exist_ok=True)
        self.model.save_pretrained(output_dir)
        if self.tokenizer is not None:
            self.tokenizer.save_pretrained(output_dir)
        tqdm.write(f"æ¨¡å‹å’Œåˆ†è¯å™¨å·²ä¿å­˜åˆ°: {output_dir}")
    def __init__(self, model, args, train_dataset=None, eval_dataset=None, tokenizer=None, data_collator=None, compute_metrics=None):
        self.model = model
        self.args = args
        self.train_dataset = train_dataset
        self.eval_dataset = eval_dataset
        self.tokenizer = tokenizer
        self.data_collator = data_collator
        self.compute_metrics = compute_metrics
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)
        # æ—¥å¿—å¹³å°åˆå§‹åŒ–
        self.report_to = getattr(args, 'report_to', None)
        self._wandb = None
        self._tb_writer = None
        if self.report_to is not None:
            if 'wandb' in self.report_to:
                try:
                    import wandb
                    wandb.init(project=getattr(args, 'wandb_project', 'my_project'), name=getattr(args, 'wandb_run_name', None))
                    self._wandb = wandb
                except ImportError:
                    print('wandb not installed, skipping wandb logging.')
            if 'tensorboard' in self.report_to:
                try:
                    from torch.utils.tensorboard import SummaryWriter
                    self._tb_writer = SummaryWriter(log_dir=getattr(args, 'tb_log_dir', './runs'))
                except ImportError:
                    print('tensorboard not installed, skipping tensorboard logging.')

    def train(self, resume_from_checkpoint=None):
        """
        è®­ç»ƒå‡½æ•°
        Args:
            resume_from_checkpoint: checkpointè·¯å¾„
            model_already_loaded: å¦‚æœä¸ºTrueï¼Œè¡¨ç¤ºæ¨¡å‹æƒé‡å·²ç»é¢„å…ˆåŠ è½½ï¼ˆå¦‚é€šè¿‡PeftModel.from_pretrainedï¼‰ï¼Œ
                                åªéœ€è¦æ¢å¤è®­ç»ƒçŠ¶æ€ï¼Œä¸å†åŠ è½½æ¨¡å‹æƒé‡
        """
        args = self.args
        # ä½¿ç”¨data_collatorï¼ˆå¦‚æœæœ‰ï¼‰
        train_loader = DataLoader(
            self.train_dataset,
            batch_size=args.train_batch_size,
            shuffle=False,
            collate_fn=self.data_collator if self.data_collator is not None else None
        )
        val_loader = DataLoader(
            self.eval_dataset,
            batch_size=args.eval_batch_size,
            collate_fn=self.data_collator if self.data_collator is not None else None
        ) if self.eval_dataset is not None else None
        
        # ====== checkpoint æ¢å¤é€»è¾‘ï¼ˆå¿…é¡»åœ¨åˆ›å»ºoptimizerä¹‹å‰ï¼‰ ======
        global_step = 0  # batch stepè®¡æ•°
        optimizer_step = 0  # optimizer stepè®¡æ•°
        start_epoch = 0
        start_step_in_epoch = 0
        
        if resume_from_checkpoint is not None:
            checkpoint_dir = resume_from_checkpoint
            print(f"å°è¯•ä» checkpoint æ¢å¤: {checkpoint_dir}")
            print("æ‰§è¡Œå®Œæ•´çš„æ¨¡å‹æƒé‡æ¢å¤...")
            # ä¿å­˜åŸå§‹æ¨¡å‹çš„é‡è¦é…ç½®
            was_gradient_checkpointing = getattr(self.model, 'gradient_checkpointing', False)
            # æ£€æŸ¥æ˜¯å¦æ˜¯ PEFT æ¨¡å‹
            is_peft_model = False
            try:
                from peft import PeftModel
                is_peft_model = isinstance(self.model, PeftModel)
                if is_peft_model:
                    print("æ£€æµ‹åˆ° PEFT æ¨¡å‹")
            except ImportError:
                pass
            # æ¢å¤æ¨¡å‹æƒé‡ - é‡‡ç”¨åŸåœ°æ›´æ–°è€Œä¸æ˜¯é‡æ–°åˆ›å»º
            try:
                if is_peft_model:
                    # PEFT æ¨¡å‹ï¼šä½¿ç”¨ç‰¹æ®Šçš„æ¢å¤æ–¹å¼
                    print("ä½¿ç”¨ PEFT æ¨¡å‹æ¢å¤æ–¹å¼")
                    base_model = self.model.get_base_model()
                    self.model = PeftModel.from_pretrained(base_model, checkpoint_dir)
                else:
                    # æ™®é€šæ¨¡å‹ï¼šå°è¯•åŸåœ°åŠ è½½æƒé‡
                    print("å°è¯•åŸåœ°åŠ è½½æ¨¡å‹æƒé‡...")
                    # ä¼˜å…ˆå°è¯•åŠ è½½ safetensors æ ¼å¼
                    weight_files = ["model.safetensors", "pytorch_model.bin"]
                    loaded = False
                    
                    for weight_file in weight_files:
                        weight_path = os.path.join(checkpoint_dir, weight_file)
                        if os.path.exists(weight_path):
                            try:
                                print(f"åŠ è½½æƒé‡æ–‡ä»¶: {weight_file}")
                                if weight_file.endswith('.safetensors'):
                                    from safetensors.torch import load_file
                                    state_dict = load_file(weight_path)
                                else:
                                    state_dict = torch.load(weight_path, map_location=self.device)
                                
                                # åŸåœ°åŠ è½½æƒé‡ï¼Œä¿æŒæ¨¡å‹ç»“æ„ä¸å˜
                                missing_keys, unexpected_keys = self.model.load_state_dict(state_dict, strict=False)
                                if missing_keys:
                                    print(f"ç¼ºå¤±çš„é”®: {len(missing_keys)} ä¸ª")
                                if unexpected_keys:
                                    print(f"æ„å¤–çš„é”®: {len(unexpected_keys)} ä¸ª")
                                print(f"âœ“ æˆåŠŸåŸåœ°åŠ è½½æƒé‡: {weight_file}")
                                loaded = True
                                break
                            except Exception as e:
                                print(f"åŠ è½½ {weight_file} å¤±è´¥: {e}")
                                continue
                    
                    if not loaded:
                        print("åŸåœ°åŠ è½½å¤±è´¥ï¼Œå°è¯• from_pretrained æ–¹å¼...")
                        self.model = self.model.from_pretrained(checkpoint_dir)
                        print("âš ï¸ ä½¿ç”¨äº† from_pretrainedï¼Œå¯èƒ½éœ€è¦é‡æ–°é…ç½®æ¨¡å‹")
                        
            except Exception as e:
                print(f"æ¨¡å‹æ¢å¤å¤±è´¥: {e}")
                print("è·³è¿‡æ¨¡å‹æ¢å¤ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹æƒé‡")
            
            # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            self.model.to(self.device)
            
            # æ¢å¤æ¢¯åº¦æ£€æŸ¥ç‚¹è®¾ç½®
            if was_gradient_checkpointing and hasattr(self.model, 'gradient_checkpointing_enable'):
                print("é‡æ–°å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
                self.model.gradient_checkpointing_enable()
            
            # æ¢å¤åˆ†è¯å™¨
            if self.tokenizer is not None:
                try:
                    self.tokenizer = self.tokenizer.from_pretrained(checkpoint_dir)
                    print("âœ“ æ¢å¤åˆ†è¯å™¨")
                except:
                    print("åˆ†è¯å™¨æ¢å¤å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹åˆ†è¯å™¨")
            
            print(f"âœ“ æ¨¡å‹æ¢å¤å®Œæˆï¼Œè®¾å¤‡: {self.model.device}")
            print(f"âœ“ æ¨¡å‹ç±»å‹: {type(self.model)}")
            if hasattr(self.model, 'gradient_checkpointing'):
                print(f"âœ“ æ¢¯åº¦æ£€æŸ¥ç‚¹: {self.model.gradient_checkpointing}")
            
            # æ¢å¤è®­ç»ƒçŠ¶æ€è®¡æ•°å™¨ï¼ˆæ— è®ºæ¨¡å‹æ˜¯å¦é¢„åŠ è½½éƒ½éœ€è¦æ¢å¤ï¼‰
            state_path = os.path.join(checkpoint_dir, "trainer_state.pt")
            if os.path.exists(state_path):
                state = torch.load(state_path, map_location="cpu")
                global_step = state.get("global_step", 0)
                optimizer_step = state.get("optimizer_step", 0)
                start_epoch = state.get("epoch", 0)
                start_step_in_epoch = state.get("step_in_epoch", 0)
                print(f"âœ“ æ¢å¤è®¡æ•°: global_step={global_step}, optimizer_step={optimizer_step}, epoch={start_epoch}, step_in_epoch={start_step_in_epoch}")
            else:
                print("æœªæ£€æµ‹åˆ° trainer_state.ptï¼Œè®¡æ•°å™¨ä½¿ç”¨åˆå§‹çŠ¶æ€")
        
        # åœ¨æ¨¡å‹æ¢å¤ååˆ›å»º optimizer/scheduler/scalerï¼ˆé‡è¦ï¼ï¼‰
        use_amp = args.fp16 or args.bf16
        scaler = torch.cuda.amp.GradScaler(enabled=args.fp16)
        optimizer = optim.AdamW(self.model.parameters(), lr=args.learning_rate)
        # è®¡ç®—çœŸå®çš„optimizer stepæ€»æ•°ï¼ˆè€ƒè™‘æ¢¯åº¦ç´¯è®¡ï¼‰
        total_batch_steps = args.num_train_epochs * len(train_loader)
        total_optimizer_steps = (total_batch_steps + args.gradient_accumulation_steps - 1) // args.gradient_accumulation_steps
        scheduler = self._create_scheduler(optimizer, total_optimizer_steps)
        progress_bar = tqdm(total=total_batch_steps, desc="Training", ncols=100)
        saved_checkpoints = []

        # æ¢å¤ optimizer/scheduler/scaler çŠ¶æ€ï¼ˆåœ¨åˆ›å»ºåï¼‰
        if resume_from_checkpoint is not None:
            checkpoint_dir = resume_from_checkpoint
            # æ¢å¤ optimizer/scheduler/scaler çŠ¶æ€
            opt_path = os.path.join(checkpoint_dir, "optimizer.pt")
            sch_path = os.path.join(checkpoint_dir, "scheduler.pt")
            scaler_path = os.path.join(checkpoint_dir, "scaler.pt")
            if os.path.exists(opt_path):
                optimizer.load_state_dict(torch.load(opt_path, map_location=self.device))
                print("âœ“ æ¢å¤ optimizer çŠ¶æ€")
            else:
                print("æœªæ£€æµ‹åˆ° optimizer.ptï¼Œoptimizer ä½¿ç”¨åˆå§‹çŠ¶æ€")
            if os.path.exists(sch_path):
                scheduler.load_state_dict(torch.load(sch_path, map_location=self.device))
                print("âœ“ æ¢å¤ scheduler çŠ¶æ€")
            else:
                print("æœªæ£€æµ‹åˆ° scheduler.ptï¼Œscheduler ä½¿ç”¨åˆå§‹çŠ¶æ€")
            if os.path.exists(scaler_path):
                scaler.load_state_dict(torch.load(scaler_path, map_location=self.device))
                print("âœ“ æ¢å¤ scaler çŠ¶æ€")
            else:
                print("æœªæ£€æµ‹åˆ° scaler.ptï¼Œscaler ä½¿ç”¨åˆå§‹çŠ¶æ€")
            
            # æ¢å¤è®¡æ•°å™¨
            state_path = os.path.join(checkpoint_dir, "trainer_state.pt")
            if os.path.exists(state_path):
                state = torch.load(state_path, map_location="cpu")
                global_step = state.get("global_step", 0)
                optimizer_step = state.get("optimizer_step", 0)
                start_epoch = state.get("epoch", 0)
                start_step_in_epoch = state.get("step_in_epoch", 0)
                print(f"âœ“ æ¢å¤è®¡æ•°: global_step={global_step}, optimizer_step={optimizer_step}, epoch={start_epoch}, step_in_epoch={start_step_in_epoch}")
            else:
                print("æœªæ£€æµ‹åˆ° trainer_state.ptï¼Œè®¡æ•°å™¨ä½¿ç”¨åˆå§‹çŠ¶æ€")
            # è¿›åº¦æ¡åŒæ­¥
            progress_bar.n = global_step
            progress_bar.last_print_n = global_step
            progress_bar.refresh()

        # è®­ç»ƒå¼€å§‹å‰çš„æ£€æŸ¥
        print(f"\n==== è®­ç»ƒå‰æ£€æŸ¥ ====")
        print(f"æ¨¡å‹è®¾å¤‡: {next(self.model.parameters()).device}")
        print(f"Trainerè®¾å¤‡: {self.device}")
        print(f"æ¨¡å‹ç±»å‹: {type(self.model)}")
        
        # æ£€æŸ¥å¯è®­ç»ƒå‚æ•°
        trainable_count = sum(1 for p in self.model.parameters() if p.requires_grad)
        total_count = sum(1 for p in self.model.parameters())
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_count} / {total_count}")
        print("==== æ£€æŸ¥å®Œæˆ ====\n")

        for epoch in range(start_epoch, args.num_train_epochs):
            epoch_loss = 0
            optimizer.zero_grad()
            loss_accumulated = 0.0
            for step, batch in enumerate(train_loader):
                # è·³è¿‡å·²å®Œæˆçš„ stepï¼ˆä»…åœ¨æ¢å¤æ—¶ç”Ÿæ•ˆï¼‰
                if epoch == start_epoch and step < start_step_in_epoch:
                    continue
                self.model.train()
                # åŠ¨æ€è·å–ä¸»è¾“å…¥å
                input_name = getattr(self.model, 'main_input_name', 'input_ids')
                model_inputs = {input_name: batch[input_name].to(self.device), 'labels': batch['labels'].to(self.device)}
                # attention_maskæ”¯æŒ
                if 'attention_mask' in batch:
                    model_inputs['attention_mask'] = batch['attention_mask'].to(self.device)
                
                # è°ƒè¯•ï¼šç¬¬ä¸€ä¸ªbatchæ£€æŸ¥è®¾å¤‡ä¸€è‡´æ€§
                if global_step == 0:
                    print(f"[DEBUG] ç¬¬ä¸€ä¸ªbatchè®¾å¤‡æ£€æŸ¥:")
                    print(f"  input_idsè®¾å¤‡: {model_inputs[input_name].device}")
                    print(f"  labelsè®¾å¤‡: {model_inputs['labels'].device}")
                    if 'attention_mask' in model_inputs:
                        print(f"  attention_maskè®¾å¤‡: {model_inputs['attention_mask'].device}")
                    print(f"  æ¨¡å‹è®¾å¤‡: {next(self.model.parameters()).device}")
                
                with torch.cuda.amp.autocast(enabled=use_amp, dtype=torch.bfloat16 if args.bf16 else torch.float16):
                    outputs = self.model(**model_inputs)
                    loss = outputs.loss / args.gradient_accumulation_steps
                if use_amp:
                    scaler.scale(loss).backward()
                else:
                    loss.backward()
                loss_accumulated += loss.item()
                if (step + 1) % args.gradient_accumulation_steps == 0 or (step + 1) == len(train_loader):
                    grad_norm = self._compute_grad_norm()
                    if use_amp:
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        optimizer.step()
                    scheduler.step()
                    optimizer.zero_grad()
                    optimizer_step += 1  # çœŸæ­£çš„optimizer stepè®¡æ•°
                epoch_loss += loss.item() * args.gradient_accumulation_steps
                global_step += 1  # batch stepè®¡æ•°
                progress_bar.update(1)
                real_epoch = epoch + (step + 1) / len(train_loader)
                progress_bar.set_postfix({
                    "ep": f"{real_epoch:.2f}/{args.num_train_epochs}",
                    "step": global_step,
                    "loss": f"{loss.item() * args.gradient_accumulation_steps:.4f}",
                    "lr": f"{scheduler.get_last_lr()[0]:.2e}"
                })
                if args.logging_steps > 0 and global_step % args.logging_steps == 0:
                    current_loss = loss_accumulated
                    # è®¡ç®—çœŸå®epochè¿›åº¦
                    real_epoch = epoch + (step + 1) / len(train_loader)
                    log_str = (
                        f"[Batch {global_step:>5}] [Opt {optimizer_step:>4}] [Ep {real_epoch:>6.3f}] | "
                        f"Loss: {current_loss:>7.4f} | GradNorm: {grad_norm:>7.3f} | LR: {scheduler.get_last_lr()[0]:.2e}"
                    )
                    tqdm.write(log_str)
                    # æ—¥å¿—ä¸ŠæŠ¥ (æŒ‰HFæ ‡å‡†æ ¼å¼)
                    if self._wandb is not None:
                        self._wandb.log({
                            'train/loss': current_loss,
                            'train/grad_norm': grad_norm,
                            'train/learning_rate': scheduler.get_last_lr()[0],
                            'train/epoch': real_epoch,
                            'train/global_step': global_step
                        }, step=global_step)
                    if self._tb_writer is not None:
                        self._tb_writer.add_scalar('train/loss', current_loss, global_step)
                        self._tb_writer.add_scalar('train/grad_norm', grad_norm, global_step)
                        self._tb_writer.add_scalar('train/learning_rate', scheduler.get_last_lr()[0], global_step)
                if (step + 1) % args.gradient_accumulation_steps == 0: 
                        loss_accumulated = 0.0
                if args.eval_strategy == "steps" and args.eval_steps > 0 and global_step % args.eval_steps == 0 and val_loader is not None:
                    val_result = self.evaluate(val_loader, desc=f"Eval@Step{global_step}")
                    if isinstance(val_result, tuple):
                        val_loss, metrics = val_result
                        metrics_str = ' | '.join([f"{k}: {float(v):.4f}" for k, v in metrics.items()]) if isinstance(metrics, dict) else str(metrics)
                        log_str = (
                            f"[Batch {global_step:>5}] [EVAL] | Loss: {val_loss:>7.4f} | {metrics_str}"
                        )
                        tqdm.write(log_str)
                        # æ—¥å¿—ä¸ŠæŠ¥ (æŒ‰HFæ ‡å‡†æ ¼å¼)
                        if self._wandb is not None:
                            log_dict = {'eval/loss': val_loss, 'train/epoch': real_epoch, 'train/global_step': global_step}
                            if isinstance(metrics, dict):
                                for k, v in metrics.items():
                                    log_dict[f'eval/{k}'] = float(v)
                            self._wandb.log(log_dict, step=global_step)
                        if self._tb_writer is not None:
                            self._tb_writer.add_scalar('eval/loss', val_loss, global_step)
                            if isinstance(metrics, dict):
                                for k, v in metrics.items():
                                    self._tb_writer.add_scalar(f'eval/{k}', float(v), global_step)
                    else:
                        tqdm.write(f"[Batch {global_step:>5}] [EVAL] | Loss: {val_result:>7.4f}")
                        if self._wandb is not None:
                            self._wandb.log({'eval/loss': val_result, 'train/epoch': real_epoch, 'train/global_step': global_step}, step=global_step)
                        if self._tb_writer is not None:
                            self._tb_writer.add_scalar('eval/loss', val_result, global_step)
                if args.save_steps > 0 and global_step % args.save_steps == 0:
                    tqdm.write(f"[Batch {global_step:>5}] [SAVE] | ä¿å­˜æ£€æŸ¥ç‚¹åˆ° checkpoint-{global_step}")
                    self._save_checkpoint(global_step, saved_checkpoints, optimizer, scheduler, scaler, epoch, step)
            avg_loss = epoch_loss / len(train_loader)
            tqdm.write(f"=== [EPOCH {epoch+1}/{args.num_train_epochs} å®Œæˆ] | å¹³å‡Loss: {avg_loss:.4f} | æ€»Batchæ­¥æ•°: {global_step} | æ€»Optæ­¥æ•°: {optimizer_step} ===")
            if args.eval_strategy == "epoch" and val_loader is not None:
                val_result = self.evaluate(val_loader, desc=f"Epoch {epoch+1}/{args.num_train_epochs} [Val]")
                if isinstance(val_result, tuple):
                    val_loss, metrics = val_result
                    tqdm.write(f"[EPOCH {epoch+1}] [EVAL] | Loss: {val_loss:.4f} | Metrics: {metrics}")
                else:
                    tqdm.write(f"[EPOCH {epoch+1}] [EVAL] | Loss: {val_result:.4f}")
            if args.save_steps == -1:
                tqdm.write(f"[EPOCH {epoch+1}] [SAVE] | ä¿å­˜epochæ£€æŸ¥ç‚¹åˆ° checkpoint-epoch{epoch+1}")
                self._save_checkpoint(f"epoch{epoch+1}", saved_checkpoints, optimizer, scheduler, scaler, epoch, 0)
        progress_bar.close()
        tqdm.write("=" * 80)
        tqdm.write(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æ€»è®¡ {args.num_train_epochs} ä¸ªepochï¼Œ{global_step} ä¸ªbatchæ­¥æ•°ï¼Œ{optimizer_step} ä¸ªä¼˜åŒ–å™¨æ­¥æ•°")
        tqdm.write("=" * 80)
        
    def evaluate(self, val_loader, desc):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œcompute_metricséœ€å¤–éƒ¨ä¼ å…¥"""
        self.model.eval()
        val_loss, predictions, references = 0, [], []
        gen_config = getattr(self.model, 'generation_config', None)
        input_name = getattr(self.model, 'main_input_name', 'input_ids')
        with torch.no_grad():
            for batch in tqdm(val_loader, desc=desc, ncols=100, leave=False):
                batch_inputs = {input_name: batch[input_name].to(self.device)}
                if 'attention_mask' in batch:
                    batch_inputs['attention_mask'] = batch['attention_mask'].to(self.device)
                lbl = batch["labels"].to(self.device)
                out = self.model(**batch_inputs, labels=lbl)
                val_loss += out.loss.item()
                if hasattr(self.model, 'generate') and gen_config is not None:
                    encoder_outputs = self.model.get_encoder()(**{input_name: batch_inputs[input_name]})
                    preds = self.model.generate(encoder_outputs=encoder_outputs, generation_config=gen_config)
                    predictions.extend(preds.cpu().tolist())
                    references.extend(lbl.cpu().tolist())
        self.model.train()
        if predictions and self.compute_metrics:
            pred = type('Pred', (), {})()
            pred.predictions, pred.label_ids = predictions, references
            return val_loss / len(val_loader), self.compute_metrics(pred)
        return val_loss / len(val_loader)

    def _save_checkpoint(self, step, saved_checkpoints, optimizer=None, scheduler=None, scaler=None, epoch=0, step_in_epoch=0):
        """ä¿å­˜æ£€æŸ¥ç‚¹ï¼ŒåŒ…æ‹¬æ¨¡å‹ã€åˆ†è¯å™¨ã€optimizerã€schedulerã€scalerã€è®¡æ•°å™¨"""
        path = os.path.join(self.args.output_dir, f"checkpoint-{step}")
        os.makedirs(path, exist_ok=True)
        self.model.save_pretrained(path)
        if self.tokenizer is not None:
            self.tokenizer.save_pretrained(path)
        # ä¿å­˜ optimizer/scheduler/scaler çŠ¶æ€
        if optimizer is not None:
            torch.save(optimizer.state_dict(), os.path.join(path, "optimizer.pt"))
        if scheduler is not None:
            torch.save(scheduler.state_dict(), os.path.join(path, "scheduler.pt"))
        if scaler is not None:
            torch.save(scaler.state_dict(), os.path.join(path, "scaler.pt"))
        # ä¿å­˜è®¡æ•°å™¨
        state = {
            "global_step": getattr(self, "global_step", 0),
            "optimizer_step": getattr(self, "optimizer_step", 0),
            "epoch": epoch,
            "step_in_epoch": step_in_epoch
        }
        torch.save(state, os.path.join(path, "trainer_state.pt"))
        saved_checkpoints.append(path)
        while self.args.save_total_limit > 0 and len(saved_checkpoints) > self.args.save_total_limit:
            shutil.rmtree(saved_checkpoints.pop(0))

    def _create_scheduler(self, optimizer, total_optimizer_steps):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        # å°†warmup_stepsä»batch stepè½¬ä¸ºoptimizer stepï¼ˆå¦‚æœéœ€è¦ï¼‰
        warmup_optimizer_steps = self.args.warmup_steps // self.args.gradient_accumulation_steps if self.args.warmup_steps > 0 else 0
        
        if self.args.lr_scheduler_type == "linear":
            return LambdaLR(optimizer, lambda s: s / max(1, warmup_optimizer_steps) if s < warmup_optimizer_steps else max(0.0, (total_optimizer_steps - s) / max(1, total_optimizer_steps - warmup_optimizer_steps)))
        if self.args.lr_scheduler_type == "cosine":
            return CosineAnnealingLR(optimizer, T_max=total_optimizer_steps, eta_min=0)
        if self.args.lr_scheduler_type == "constant":
            return LambdaLR(optimizer, lambda _: 1.0)
        raise ValueError(f"Unsupported lr_scheduler_type: {self.args.lr_scheduler_type}")

    def _compute_grad_norm(self):
        """è®¡ç®—æ¨¡å‹æ¢¯åº¦èŒƒæ•°"""
        total_norm = 0.0
        param_count = 0
        for p in self.model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
                param_count += 1
        
        if param_count == 0:
            return 0.0
        return total_norm ** 0.5

import os
import shutil
from tqdm import tqdm
import torch
from torch.utils.data import DataLoader
import torch.optim as optim
from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR

from dataclasses import dataclass, field

@dataclass
class MySeq2SeqTrainingArguments:
    output_dir: str = 'VIT_GPT2_EDM'
    train_batch_size: int = 8
    eval_batch_size: int = 8
    eval_strategy: str = "steps"
    eval_steps: int = 128
    logging_steps: int = 128
    save_steps: int = 2048
    warmup_steps: int = 1024
    learning_rate: float = 5e-5
    num_train_epochs: int = 3
    save_total_limit: int = 1
    lr_scheduler_type: str = "linear"
    gradient_accumulation_steps: int = 1
    fp16: bool = False
    bf16: bool = False
    # æ—¥å¿—å¹³å°å‚æ•°
    report_to: list = field(default_factory=list)  # e.g. ["wandb", "tensorboard"]
    wandb_project: str = 'my_project'
    wandb_run_name: str = None
    tb_log_dir: str = './runs'
